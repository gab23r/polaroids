{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Polars on steroids!","text":"<p>This package provides a generic extension to Polars <code>DataFrame</code>, allowing data validation and typing goodies.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Generic DataFrame: Ensures type safety using Python's <code>TypedDict</code>.</li> <li>Data Validation: Checks that the DataFrame conforms to the expected schema.</li> <li>Custom Checks: Leverage the power of polars expression to add custom checks.</li> <li>Lightweight: No dependencies (except polars)!</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install polaroids\n</code></pre>"},{"location":"#get-started","title":"Get Started","text":""},{"location":"#defining-a-schema","title":"Defining a Schema","text":"<p>Schemas are defined using Python's <code>TypedDict</code>:</p> <pre><code>from typing import Annotated, TypedDict\nfrom polaroids import DataFrame, Field\nimport polars as pl\n\nclass BasicSchema(TypedDict):\n    a: Annotated[int, Field(\n        sorted=\"ascending\",\n        coerce=True,\n        unique=True,\n        checks=[lambda d: d.ge(0)],\n    )]\n    b: int | None\n\ndf = pl.DataFrame({\"a\": [0.0, 1.0], \"b\": [None, 0]})\n\nDataFrame[BasicSchema](df).validate()\n# shape: (2, 2)\n# \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n# \u2502 a   \u2506 b    \u2502\n# \u2502 --- \u2506 ---  \u2502\n# \u2502 i64 \u2506 i64  \u2502\n# \u255e\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n# \u2502 0   \u2506 null \u2502\n# \u2502 1   \u2506 0    \u2502\n# \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"#get-typing-goodies","title":"Get typing goodies!","text":"<p>Get your TypedDict back when you leave polars \u2705:</p> <p></p>"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#polaroids.DataFrame","title":"<code>DataFrame</code>","text":"<p>               Bases: <code>DataFrame</code>, <code>Generic[S]</code></p> <p>A generic Polars DataFrame with schema validation.</p> <p>This class extends <code>polars.DataFrame</code> to support schema validation using Python's type annotations and metadata. It ensures that the DataFrame conforms to a specified schema, enforcing constraints such as sorting, uniqueness, and custom validation checks.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The input DataFrame to be validated.</p> required Type Parameters <p>S : TypedDict     The schema definition as a <code>TypedDict</code>, where fields can have metadata     such as sorting, uniqueness, coercion, and validation checks.</p> <p>Methods:</p> Name Description <code>validate</code> <p>Validates the DataFrame against the expected schema.</p> Example <pre><code>from typing import Annotated, TypedDict\nfrom polaroids import DataFrame, Field\nfrom polaroids.types import int32\nimport polars as pl\n\n\nclass BasicSchema(TypedDict):\n    a: Annotated[\n        int32,\n        Field(\n            sorted=\"ascending\",\n            coerce=True,\n            unique=True,\n            checks=[lambda d: d.ge(0)],  # Ensures values are non-negative\n        ),\n    ]\n    b: int | None  # Optional integer column\n\n\ndf = pl.DataFrame({\"a\": [0.0, 1.0], \"b\": [None, 0]})\nvalidated_df = DataFrame[BasicSchema](df).validate()\n</code></pre> <p>The <code>validate()</code> method ensures that: - The schema of <code>df</code> matches the TypedDict (with possible coercion). - Column <code>a</code> is sorted in ascending order. - Column <code>a</code> only contains non-negative values. - Column <code>a</code> has unique values. - Column <code>b</code> allows <code>None</code> values.</p> <p>Raises:</p> Type Description <code>ValidationError</code> <p>If the DataFrame does not conform to the expected schema.</p>"},{"location":"api/#polaroids.DataFrame.__getattribute__","title":"<code>__getattribute__(name)</code>","text":"<p>Dynamically delegate attribute access to the underlying <code>polars.DataFrame</code>.</p> <p>This method intercepts attribute lookups that are not found on <code>DataFrame</code> and attempts to retrieve them from the <code>polars.DataFrame</code> superclass, the restult is converted back into an instance of this <code>DataFrame</code> subclass.</p> <p>We intercept only on subset of polars.DataFrame methods, we intercept only methods that might not change the schema.</p> Source code in <code>src/polaroids/dataframe.py</code> <pre><code>def __getattribute__(self, name: str):\n    \"\"\"Dynamically delegate attribute access to the underlying `polars.DataFrame`.\n\n    This method intercepts attribute lookups that are not found on `DataFrame`\n    and attempts to retrieve them from the `polars.DataFrame` superclass, the restult is converted back into an instance\n    of this `DataFrame` subclass.\n\n    We intercept only on subset of polars.DataFrame methods, we intercept only methods that might not change the schema.\n    \"\"\"\n    if name in [\n        \"head\",\n        \"limit\",\n        \"filter\",\n        \"slice\",\n        \"sort\",\n        \"drop_nulls\",\n        \"unique\",\n        \"fill_null\",\n        \"fill_nan\",\n        \"with_columns\",\n        \"select\",\n        \"cast\",\n    ]:\n        attr = getattr(super(), name)  # Get the attribute from `pl.DataFrame`\n\n        def wrapper(*args, **kwargs):\n            result = attr(*args, **kwargs)  # Call the method\n            new = self.__class__(result)\n            setattr(new, \"__orig_class__\", getattr(self, \"__orig_class__\", None))\n            return new\n\n        return wrapper\n\n    return super().__getattribute__(name)  # Get the original method from `pl.DataFrame`\n</code></pre>"},{"location":"api/#polaroids.DataFrame.validate","title":"<code>validate()</code>","text":"<p>Validate the dataframe based on the annotations of the TypedDict.</p> <p>This function performs various validation checks, including:</p> <ul> <li>Schema equality: Ensures that the DataFrame matches the expected schema.</li> <li>Primary key uniqueness: Verifies that primary key columns contain unique values.</li> <li>Unique values: Checks for unique constraints on specific columns.</li> <li>Nullable columns: Ensures that required columns do not contain null values.</li> <li>Sortedness: Validates whether specified columns are sorted in the expected order.</li> <li>Custom checks: Applies user-defined validation functions.</li> </ul> <p>Returns:</p> Type Description <code>    Self: The validated DataFrame.</code> <p>Raises:</p> Type Description <code>    ValidationError: If any validation check fails.</code> Source code in <code>src/polaroids/dataframe.py</code> <pre><code>def validate(self: Self) -&gt; Self:\n    \"\"\"Validate the dataframe based on the annotations of the TypedDict.\n\n    This function performs various validation checks, including:\n\n    - **Schema equality**: Ensures that the DataFrame matches the expected schema.\n    - **Primary key uniqueness**: Verifies that primary key columns contain unique values.\n    - **Unique values**: Checks for unique constraints on specific columns.\n    - **Nullable columns**: Ensures that required columns do not contain null values.\n    - **Sortedness**: Validates whether specified columns are sorted in the expected order.\n    - **Custom checks**: Applies user-defined validation functions.\n\n    Returns\n    -------\n        Self: The validated DataFrame.\n\n    Raises\n    ------\n        ValidationError: If any validation check fails.\n    \"\"\"\n    # Coerce\n    if coerce_cols := self._metadata.filter(pl.col(\"coerce\"))[\"column\"].to_list():\n        self = self.cast({c: dtype for c, dtype in self._schema.items() if c in coerce_cols})  # type: ignore\n\n    _utils.assert_schema_equal(self._schema, self.schema)\n\n    # Reorder columns\n    self = self.select(self._schema.keys())  # type: ignore\n\n    # Nullable\n    if non_nullable_cols := self._metadata.filter(~pl.col(\"nullable\"))[\"column\"].to_list():\n        if is_null := (\n            self.select(pl.col(non_nullable_cols).is_null().any())\n            .transpose(include_header=True, column_names=[\"is_null\"])\n            .filter(pl.col(\"is_null\"))\n            .get_column(\"column\")\n            .to_list()\n        ):\n            raise ValidationError(f\"The following columns contains nulls: {is_null}.\")\n\n    # Uniqueness\n    if unique_cols := self._metadata.filter(pl.col(\"unique\"))[\"column\"].to_list():\n        if is_duplicated := (\n            self.select(pl.col(unique_cols).is_duplicated().any())\n            .transpose(include_header=True, column_names=[\"is_duplicated\"])\n            .filter(pl.col(\"is_duplicated\"))\n            .get_column(\"column\")\n            .to_list()\n        ):\n            raise ValidationError(\n                f\"The following columns must be unique but contain duplicates: {is_duplicated}.\"\n            )\n\n    # Primary key\n    if pk_cols := self._metadata.filter(pl.col(\"primary_key\"))[\"column\"].to_list():\n        df_duplicated = self.select(pk_cols).filter(pl.struct(pk_cols).is_duplicated())\n        if df_duplicated.height:\n            raise ValidationError(f\"Primary key constraint violated:\\n{df_duplicated}.\")\n\n    # Is sorted\n    for descending, columns in (\n        self._metadata.filter(pl.col(\"sorted\").is_not_null())\n        .group_by(descending=pl.col(\"sorted\").eq(\"descending\"))\n        .agg(\"column\")\n        .iter_rows()\n    ):\n        for column in columns:\n            if not self.get_column(column).is_sorted(descending=descending):\n                raise ValidationError(\n                    f\"Column {column!r} is not sorted as expected (descending={descending}).\"\n                )\n        self = self.with_columns(pl.col(columns).set_sorted(descending=descending))  # type: ignore\n\n    # Custom checks\n    for column, checks in (\n        self._metadata.select(\"column\", \"checks\").filter(pl.col(\"checks\").is_not_null()).rows()\n    ):\n        result = self.select(\n            [check(pl.col(column)).alias(str(i)) for i, check in enumerate(checks)]\n        )\n        for i, check_ok in result.select(pl.all().all()).row(0, named=True).items():\n            if not check_ok:\n                df_failure = self.filter(result.get_column(i))\n                raise ValidationError(\n                    f\"Check number {i} on column {column!r} fails:\\n{df_failure}.\"\n                )\n\n    return self\n</code></pre>"},{"location":"api/#polaroids.Field","title":"<code>Field</code>","text":"<p>               Bases: <code>TypedDict</code></p> <p>TypedDict representing the configuration for a field in a schema.</p> <p>Attributes:</p> Name Type Description <code>primary_key</code> <code>bool</code> <p>Indicates whether the field is a primary key.</p> <code>unique</code> <code>bool</code> <p>Indicates whether the field values must be unique.</p> <code>sorted</code> <code>{descending, ascending}</code> <p>Specifies the sorting order for the field.</p> <code>coerce</code> <code>bool</code> <p>Indicates whether to coerce the field values to the specified type.</p> <code>default</code> <code>Expr</code> <p>The default value for the field.</p> <code>checks</code> <code>list[Callable[[Expr], Expr]]</code> <p>A list of validation checks for the field.</p>"}]}